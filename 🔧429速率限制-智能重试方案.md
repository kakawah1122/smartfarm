# 🔧 429速率限制 - 智能重试方案

## 📊 问题诊断

### 错误信息
```
Request failed with status code 429
```

### 根本原因
- **HTTP 429 = Too Many Requests（请求过于频繁）**
- 智谱API的免费额度有速率限制（QPM限制）
- 之前测试太频繁，触发了速率限制

### 🎉 好消息
**从400错误变成429错误，说明GLM-4V的调用方式是正确的！** 只是遇到了速率限制问题。

---

## ✅ 解决方案 - 智能重试和Fallback

### 核心思路
1. **主模型**: GLM-4V（智谱AI多模态视觉模型）
2. **备用模型**: Qwen-VL（SiliconFlow多模态模型）
3. **智能重试**: 遇到429自动切换到备用模型
4. **等待机制**: 在切换前等待2秒

### 代码实现

#### 1. `ai-multi-model/index.js` - 添加429错误检测
```javascript
// 调用AI模型（带重试）
async callModel(modelId, messages, options = {}) {
  // ... 原有代码 ...
  
  try {
    // ... API调用 ...
  } catch (error) {
    // 如果是429错误（速率限制），添加特殊标记
    if (error.response?.status === 429) {
      error.isRateLimited = true
      error.retryAfter = error.response?.headers?.['retry-after'] || 60
    }
    throw error
  }
}
```

#### 2. `handleChatCompletion` - 智能重试逻辑
```javascript
async function handleChatCompletion(event, manager) {
  try {
    // 获取模型配置
    const modelMapping = TASK_MODEL_MAPPING[actualTaskType]
    
    // 准备尝试的模型列表
    const modelsToTry = [modelMapping.primary]
    if (modelMapping.fallback) {
      modelsToTry.push(modelMapping.fallback)
    }

    // 依次尝试每个模型
    let lastError = null
    for (let i = 0; i < modelsToTry.length; i++) {
      const modelId = modelsToTry[i]
      
      try {
        // 调用模型
        const result = await manager.callModel(modelId, processedMessages, options)
        
        // 成功，返回结果
        return {
          success: true,
          data: result,
          modelUsed: modelId,
          usedVision,
          fromCache: false
        }
      } catch (error) {
        lastError = error
        
        // 如果是速率限制且还有其他模型可尝试
        if (error.isRateLimited && i < modelsToTry.length - 1) {
          console.log(`⚠️ 速率限制，等待2秒后尝试下一个模型...`)
          await new Promise(resolve => setTimeout(resolve, 2000))
          continue
        }
        
        // 尝试下一个模型
        if (i < modelsToTry.length - 1) {
          continue
        }
        
        // 所有模型都失败了
        throw error
      }
    }
  } catch (error) {
    // 返回友好错误信息
  }
}
```

### 模型配置

```javascript
// 视觉诊断任务的模型配置
TASK_MODEL_MAPPING = {
  health_diagnosis_vision: {
    primary: 'glm-4v',      // 主模型：智谱GLM-4V
    fallback: 'qwen-vl'     // 备用：Qwen-VL
  }
}
```

---

## 🚀 部署步骤

### 1. 上传云函数
```bash
# 在微信开发者工具中右键 ai-multi-model
选择：上传并部署：云端安装依赖（不上传node_modules）
```

### 2. 测试流程

#### 场景1：主模型可用
```
开始处理对话
→ 尝试模型 1/2: glm-4v
→ ✅ 模型 glm-4v 调用成功
→ 返回诊断结果
```

#### 场景2：主模型速率限制
```
开始处理对话
→ 尝试模型 1/2: glm-4v
→ ❌ 模型 glm-4v 失败: 429
→ ⚠️ 速率限制，等待2秒...
→ 尝试模型 2/2: qwen-vl
→ ✅ 模型 qwen-vl 调用成功
→ 返回诊断结果
```

#### 场景3：所有模型都失败
```
开始处理对话
→ 尝试模型 1/2: glm-4v
→ ❌ 模型 glm-4v 失败: 429
→ 尝试模型 2/2: qwen-vl
→ ❌ 模型 qwen-vl 失败: XXX
→ 返回错误: 所有模型调用均失败
```

---

## 🔍 日志监控

### 关键日志

1. **模型选择日志**
```
尝试模型 1/2: glm-4v (智谱GLM-4V)
```

2. **速率限制日志**
```
⚠️ 速率限制，等待2秒后尝试下一个模型...
```

3. **成功日志**
```
✅ 模型 glm-4v 调用成功
```

4. **失败日志**
```
❌ 模型 glm-4v 失败: Request failed with status code 429
```

### 查看云函数日志
```
微信开发者工具
→ 云开发控制台
→ 云函数
→ ai-multi-model
→ 日志
```

---

## 💡 优化建议

### 1. 避免速率限制
- **减少测试频率**：不要连续快速测试
- **等待时间**：遇到429后至少等待1分钟
- **升级套餐**：考虑购买更高QPM限制的套餐

### 2. 优化调用策略
- **缓存诊断结果**（已实现）
- **批量处理**：多个症状一起诊断
- **延迟调用**：非紧急诊断可延迟

### 3. 监控和告警
- 记录每日API调用次数
- 统计429错误频率
- 设置额度预警

---

## 📋 测试清单

- [ ] 部署 `ai-multi-model` 云函数
- [ ] 上传3张症状图片
- [ ] 提交AI诊断
- [ ] 查看云函数日志
- [ ] 确认使用了哪个模型（glm-4v 或 qwen-vl）
- [ ] 检查诊断结果是否包含图片分析

---

## ⚠️ 注意事项

1. **速率限制恢复时间**
   - 通常1分钟后自动恢复
   - 严重情况可能需要等待更长时间

2. **备用模型差异**
   - GLM-4V 和 Qwen-VL 的诊断结果可能略有不同
   - 都是高质量的多模态模型

3. **错误信息传递**
   - 前端会显示详细的错误信息
   - 包括尝试了哪些模型
   - 失败原因

---

## 🎯 预期效果

### 成功率提升
- **之前**: 遇到429直接失败
- **现在**: 自动切换到备用模型，成功率接近100%

### 用户体验
- **无感切换**: 用户不需要关心使用了哪个模型
- **快速响应**: 等待时间仅增加2秒
- **友好提示**: 失败时提供明确的错误信息

---

## 📞 技术支持

如果遇到问题，请提供以下信息：
1. 云函数日志截图
2. 前端错误信息
3. 诊断任务ID
4. 上传的图片数量

---

**✅ 现在可以重新部署并测试了！**

